{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") # This is the api key for openai\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\") # This is the api key for groq\n",
    "\n",
    "import os\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kush/Desktop/Live Agentic AI Course 2.0/env/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use mps:0\n",
      "/var/folders/r3/7drjzfcx4nj51lgcbzhsctm80000gn/T/ipykernel_27623/3217946123.py:5: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is agentic ai?\n",
      "\n",
      "Ai is an acronym for \"Agent of the Universe,\" which means that we can think of an object that is created at some point in time, and then is then destroyed or destroyed again at some point in time, or that we can think of as a universe. That is, an agent created at some point in time has an agentic quality. When we think of an object of an agentic quality, we are talking about an object that is created at some point and then destroyed or destroyed again.\n",
      "\n",
      "It is important to note that agents are independent entities. If we are talking about an object and cannot think of it at any other time than it exists in the universe, then we are not talking about an agent.\n",
      "\n",
      "For example, if we are talking about an object and cannot think of it at any other time than it exists in the universe, then we are not talking about an agent with an agentic quality.\n",
      "\n",
      "What is agentic?\n",
      "\n",
      "Ai refers to an object that is created at some point and then is destroyed or destroyed again at some point in time. This is not an agent or an agentic quality, but an agentic quality.\n",
      "\n",
      "An agent is a thing that is created at some point in\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "hf_pipeline = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
    "\n",
    "result = llm.invoke(\"What is agentic ai?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now similarly we will use openai \n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm2 = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "result2 = llm2.invoke(\"What is the capital of France?\")\n",
    "print(result2)# Now similarly we will use openai \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will use groq\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "llm3 = ChatGroq(model=\"llama3-8b-8192\")\n",
    "result3 = llm3.invoke(\"What is agentic ai?\")\n",
    "print(result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result3.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model = ChatGroq(model='qwen-qwq-32b')\n",
    "model.invoke('Who is the pm of India')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.invoke('What is prompt engineering? ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate \n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        # Now firstly we will define the System Prompt\n",
    "        (\"system\",\"You are an Expert AI engineer, provide me answer based on the question\"),\n",
    "        (\"user\",\"{input}\") # User will be going to give the input -> {input} (in this place holder)\n",
    "        # Instead of \"user\" we can also write \"human\"   \n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model = ChatGroq(model=\"gemma2-9b-it\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chaining\n",
    "\n",
    "chain = prompt | model \n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke(\n",
    "    {\"input\" : \"Can u tell me something about Langsmith\"}\n",
    ")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Now using the StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model | parser\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_good_response = chain.invoke(\n",
    "    {\"input\" : \"What is the difference between gen ai and Agentic AI with real life examples\"}\n",
    ")\n",
    "print(a_good_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now using JsonOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_groq import ChatGroq  # or ChatOpenAI, etc.\n",
    "\n",
    "# 1. Define the prompt (with escaped curly braces)\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert AI assistant. Always respond in the following JSON format: {{\\\"summary\\\": <summary>, \\\"keywords\\\": [<keyword1>, <keyword2>, ...]}}\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "# 2. Set up the model\n",
    "model = ChatGroq(model=\"gemma2-9b-it\")  # or ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# 3. Add the JSON output parser\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "# 4. Chain them together\n",
    "chain = prompt | model | parser\n",
    "\n",
    "# 5. Invoke the chain\n",
    "response = chain.invoke({\"input\": \"Tell me about the benefits of regular exercise.\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Learnings from above code\n",
    "\n",
    "1. Why are we using double curly braces ({{ and }})?\n",
    "    - In LangChain prompt templates, single curly braces {} are used for variable substitution (e.g., {input}).\n",
    "    - If you want to include a literal curly brace in your prompt (for example, to show a JSON example), you must escape it by doubling it: {{ and }}.\n",
    "2. Why use <summary>, <keyword1>, etc.?\n",
    "    - They are not real variables or code.\n",
    "    - They are just a way to show the LLM (and the human reading the prompt) what kind of value should go in that spot.\n",
    "    - It’s a common way to indicate “replace this with the actual content.”\n",
    "3. Why use \\\"summary\\\" (with the backslash)?\n",
    "    - The Backslash (\\) in Strings\n",
    "    - In Python, a backslash is used as an escape character.\n",
    "    - But if you want to include double quotes inside a string that is itself delimited by double quotes, you need to escape them:\n",
    "    - Otherwise, Python would get confused about where your string starts and ends.\n",
    "\n",
    "<> - They are just a way to show the LLM (and the human reading the prompt) what kind of value should go in that spot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_groq import ChatGroq  # or ChatOpenAI, etc.\n",
    "\n",
    "# 1. Define the prompt (with escaped curly braces)\n",
    "template = (\n",
    "    \"You are an expert AI assistant. \"\n",
    "    \"Always respond in the following JSON format: \"\n",
    "    \"{{\\\"summary\\\": <summary>, \\\"keywords\\\": [<keyword1>, <keyword2>, ...]}}\\n\"\n",
    "    \"Question: {input}\"\n",
    ")\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# 2. Set up the model\n",
    "model = ChatGroq(model=\"gemma2-9b-it\")  # or ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# 3. Add the JSON output parser\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "# 4. Chain them together\n",
    "chain = prompt | model | parser\n",
    "\n",
    "# 5. Invoke the chain\n",
    "response = chain.invoke({\"input\": \"Tell me about the benefits of regular exercise.\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = JsonOutputParser()\n",
    "print(output_parser.get_format_instructions())\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user queey \\n {format_instructions} \\n {query}\",\n",
    "    input_variables= [\"query\"], # The input_variables parameter in PromptTemplate is a list of variable names that your template expects to receive as input when you use it.\n",
    "    partial_variables= {\"format_instructions\" : output_parser.get_format_instructions() },\n",
    ")\n",
    "model = ChatGroq(model=\"gemma2-9b-it\") \n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "response = chain.invoke({\"query\": \"Tell me about the benefits of regular exercise.\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# 1. Create the output parser\n",
    "output_parser = JsonOutputParser()\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "# 2. Create the chat prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", f\"Answer the user query. {format_instructions}\"),\n",
    "    (\"user\", \"{query}\")\n",
    "])\n",
    "\n",
    "# 3. Set up the model\n",
    "model = ChatGroq(model=\"gemma2-9b-it\")\n",
    "\n",
    "# 4. Chain them together (optional: add the parser if you want parsed output)\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "# 5. Invoke the chain\n",
    "response = chain.invoke({\"query\": \"Tell me about the benefits of regular exercise.\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_instructions = output_parser.get_format_instructions()\n",
    "format_instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid template: {'input'}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m output_parser = XMLOutputParser()\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Fix prompt: explicitly guide the XML structure\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m prompt = \u001b[43mChatPromptTemplate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_messages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mYou are an AI Expert and you have to provide the respond on the basis of this XML Format: <response><answer> Your Answer Here </response></answer>\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Model setup\u001b[39;00m\n\u001b[32m     17\u001b[39m model = ChatGroq(model=\u001b[33m\"\u001b[39m\u001b[33mmeta-llama/llama-4-scout-17b-16e-instruct\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# No model called 'gemma2-9b-it' exists on Groq at the moment\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Live Agentic AI Course 2.0/env/lib/python3.13/site-packages/langchain_core/prompts/chat.py:1167\u001b[39m, in \u001b[36mChatPromptTemplate.from_messages\u001b[39m\u001b[34m(cls, messages, template_format)\u001b[39m\n\u001b[32m   1127\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m   1128\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_messages\u001b[39m(\n\u001b[32m   1129\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   1130\u001b[39m     messages: Sequence[MessageLikeRepresentation],\n\u001b[32m   1131\u001b[39m     template_format: PromptTemplateFormat = \u001b[33m\"\u001b[39m\u001b[33mf-string\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1132\u001b[39m ) -> ChatPromptTemplate:\n\u001b[32m   1133\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Create a chat prompt template from a variety of message formats.\u001b[39;00m\n\u001b[32m   1134\u001b[39m \n\u001b[32m   1135\u001b[39m \u001b[33;03m    Examples:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1165\u001b[39m \u001b[33;03m        a chat prompt template.\u001b[39;00m\n\u001b[32m   1166\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1167\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemplate_format\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Live Agentic AI Course 2.0/env/lib/python3.13/site-packages/langchain_core/prompts/chat.py:950\u001b[39m, in \u001b[36mChatPromptTemplate.__init__\u001b[39m\u001b[34m(self, messages, template_format, **kwargs)\u001b[39m\n\u001b[32m    895\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m    896\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    897\u001b[39m     messages: Sequence[MessageLikeRepresentation],\n\u001b[32m   (...)\u001b[39m\u001b[32m    900\u001b[39m     **kwargs: Any,\n\u001b[32m    901\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    902\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Create a chat prompt template from a variety of message formats.\u001b[39;00m\n\u001b[32m    903\u001b[39m \n\u001b[32m    904\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    947\u001b[39m \n\u001b[32m    948\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    949\u001b[39m     _messages = [\n\u001b[32m--> \u001b[39m\u001b[32m950\u001b[39m         \u001b[43m_convert_to_message_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    951\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m messages\n\u001b[32m    952\u001b[39m     ]\n\u001b[32m    954\u001b[39m     \u001b[38;5;66;03m# Automatically infer input variables from messages\u001b[39;00m\n\u001b[32m    955\u001b[39m     input_vars: \u001b[38;5;28mset\u001b[39m[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Live Agentic AI Course 2.0/env/lib/python3.13/site-packages/langchain_core/prompts/chat.py:1443\u001b[39m, in \u001b[36m_convert_to_message_template\u001b[39m\u001b[34m(message, template_format)\u001b[39m\n\u001b[32m   1441\u001b[39m message_type_str, template = message\n\u001b[32m   1442\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message_type_str, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1443\u001b[39m     _message = \u001b[43m_create_template_from_message_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1444\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessage_type_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemplate_format\u001b[49m\n\u001b[32m   1445\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1446\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1447\u001b[39m     _message = message_type_str(\n\u001b[32m   1448\u001b[39m         prompt=PromptTemplate.from_template(\n\u001b[32m   1449\u001b[39m             cast(\u001b[33m\"\u001b[39m\u001b[33mstr\u001b[39m\u001b[33m\"\u001b[39m, template), template_format=template_format\n\u001b[32m   1450\u001b[39m         )\n\u001b[32m   1451\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Live Agentic AI Course 2.0/env/lib/python3.13/site-packages/langchain_core/prompts/chat.py:1342\u001b[39m, in \u001b[36m_create_template_from_message_type\u001b[39m\u001b[34m(message_type, template, template_format)\u001b[39m\n\u001b[32m   1328\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Create a message prompt template from a message type and template string.\u001b[39;00m\n\u001b[32m   1329\u001b[39m \n\u001b[32m   1330\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1339\u001b[39m \u001b[33;03m    ValueError: If unexpected message type.\u001b[39;00m\n\u001b[32m   1340\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1341\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m message_type \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mhuman\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1342\u001b[39m     message: BaseMessagePromptTemplate = \u001b[43mHumanMessagePromptTemplate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_template\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1343\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemplate_format\u001b[49m\n\u001b[32m   1344\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1345\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m message_type \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mai\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33massistant\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1346\u001b[39m     message = AIMessagePromptTemplate.from_template(\n\u001b[32m   1347\u001b[39m         cast(\u001b[33m\"\u001b[39m\u001b[33mstr\u001b[39m\u001b[33m\"\u001b[39m, template), template_format=template_format\n\u001b[32m   1348\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Live Agentic AI Course 2.0/env/lib/python3.13/site-packages/langchain_core/prompts/chat.py:528\u001b[39m, in \u001b[36m_StringImageMessagePromptTemplate.from_template\u001b[39m\u001b[34m(cls, template, template_format, partial_variables, **kwargs)\u001b[39m\n\u001b[32m    526\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(prompt=prompt, **kwargs)\n\u001b[32m    527\u001b[39m msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid template: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m528\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[31mValueError\u001b[39m: Invalid template: {'input'}"
     ]
    }
   ],
   "source": [
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_core.output_parsers import XMLOutputParser\n",
    "# from langchain_groq import ChatGroq\n",
    "\n",
    "# # Define XML parser\n",
    "# output_parser = XMLOutputParser()\n",
    "\n",
    "# # Fix prompt: explicitly guide the XML structure\n",
    "# prompt = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\"system\",\"You are an AI Expert and you have to provide the respond on the basis of this XML Format: <response><answer> Your Answer Here </response></answer>\"),\n",
    "#         (\"user\",{\"input\"})\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# # Model setup\n",
    "# model = ChatGroq(model=\"meta-llama/llama-4-scout-17b-16e-instruct\")  # No model called 'gemma2-9b-it' exists on Groq at the moment\n",
    "\n",
    "# # Combine chain\n",
    "# chain = prompt | model \n",
    "\n",
    "# # Run the chain\n",
    "# response = chain.invoke({\"input\": \"What is LangChain?\"})\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
